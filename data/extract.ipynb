{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Rajib\n",
    "# This program shows how to do context aware parsing of a large PDF and then summarize it\n",
    "# References:\n",
    "# https://python.langchain.com/docs/use_cases/summarization\n",
    "# https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/\n",
    "# https://smith.langchain.com/hub/\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from adobe.pdfservices.operation.auth.credentials import Credentials\n",
    "from adobe.pdfservices.operation.execution_context import ExecutionContext\n",
    "from adobe.pdfservices.operation.io.file_ref import FileRef\n",
    "from adobe.pdfservices.operation.pdfops.extract_pdf_operation import ExtractPDFOperation\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_element_type import ExtractElementType\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.table_structure_type import TableStructureType\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_pdf_options import ExtractPDFOptions\n",
    "from adobe.pdfservices.operation.pdfops.options.extractpdf.extract_renditions_element_type import ExtractRenditionsElementType\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain, ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", \"INFO\"))\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "class PDFExtract():\n",
    "    def __init__(self, client_id, client_secret):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        # initialize pinecone\n",
    "        # One thing I dound out(may be a defect in PINECONE), the api_key and the environment must be provided as below\n",
    "        \n",
    "\n",
    "    def _get_credentials(self):\n",
    "        credentials = Credentials.service_principal_credentials_builder().with_client_id(\n",
    "            self.client_id).with_client_secret(self.client_secret).build()\n",
    "\n",
    "        return credentials\n",
    "\n",
    "    def _zip_file(self, output_path,unzip_dir):\n",
    "        with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_dir)\n",
    "\n",
    "    def _parse_json(self, json_file_path):\n",
    "        with open(json_file_path, \"r\") as json_file:\n",
    "            content = json.loads(json_file.read())\n",
    "\n",
    "        pdf_element = content[\"elements\"]\n",
    "        return pdf_element\n",
    "\n",
    "    # def load_pine_index(self,docs,index_name = \"arxiv-index\"):\n",
    "\n",
    "    #     emb = OpenAIEmbeddings()\n",
    "    #     docsearch = Pinecone.from_documents(docs, emb, index_name=index_name)\n",
    "\n",
    "    def get_files_from_dir(self, dir):\n",
    "        file_list = []\n",
    "        files = [os.path.join(dir, f) for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
    "\n",
    "        return files\n",
    "\n",
    "    def load_docs(self, file_path):\n",
    "        loader = TextLoader(file_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def parse_pdf(self, input_file_path, output_path, unzip_dir, chunked_dir):\n",
    "        try:\n",
    "            credentials = self._get_credentials()\n",
    "            execution_context = ExecutionContext.create(credentials)\n",
    "            extract_pdf_operation = ExtractPDFOperation.create_new()\n",
    "            source = FileRef.create_from_local_file(input_file_path)\n",
    "            extract_pdf_operation.set_input(source)\n",
    "            extract_pdf_options: ExtractPDFOptions = ExtractPDFOptions.builder() \\\n",
    "                .with_element_to_extract(ExtractElementType.TEXT) \\\n",
    "                .with_element_to_extract(ExtractElementType.TABLES) \\\n",
    "                .with_table_structure_format(TableStructureType.CSV) \\\n",
    "                .with_element_to_extract_renditions(ExtractRenditionsElementType.FIGURES)\\\n",
    "                .build()\n",
    "            extract_pdf_operation.set_options(extract_pdf_options)\n",
    "\n",
    "            # Execute the operation.\n",
    "            result: FileRef = extract_pdf_operation.execute(execution_context)\n",
    "\n",
    "            # # Save the result to the specified location.\n",
    "            result.save_as(output_path)\n",
    "            self._zip_file(output_path,unzip_dir)\n",
    "            json_file_path = os.path.join(unzip_dir, \"structuredData.json\")\n",
    "            elements = self._parse_json(json_file_path)\n",
    "\n",
    "            file_split = 0\n",
    "            # Define the header flag. If first time header no need to cut a new file\n",
    "            FIRST_TIME_HEADER = True\n",
    "            file_name = os.path.join(chunked_dir, f\"file_{file_split}\".format(file_split=file_split))\n",
    "            parsed_file = open(file_name, \"a\", encoding=\"utf-8\")\n",
    "            for element in elements:\n",
    "                if \"//Document/H2\" in element[\"Path\"]:\n",
    "                    hdr_txt = element[\"Text\"]\n",
    "                    if FIRST_TIME_HEADER:\n",
    "                        FIRST_TIME_HEADER = False\n",
    "                        parsed_file.write(hdr_txt)\n",
    "                        parsed_file.write(\"\\n\")\n",
    "                    else:\n",
    "                        parsed_file.close()\n",
    "                        file_split = file_split + 1\n",
    "                        file_name = os.path.join(chunked_dir, f\"file_{file_split}\".format(file_split=file_split))\n",
    "                        parsed_file = open(file_name, \"a\", encoding=\"utf-8\")\n",
    "                        parsed_file.write(hdr_txt)\n",
    "                        parsed_file.write(\"\\n\")\n",
    "                else:\n",
    "                    if \"Document/Table\" in element[\"Path\"]:\n",
    "                        match = re.search(r'^//Document/Table(?:\\[\\d+\\])?$', element[\"Path\"])\n",
    "                        if match:\n",
    "                            xlsx_file_name = element[\"filePaths\"][0]\n",
    "                            xlsx_file = os.path.join(unzip_dir, xlsx_file_name)\n",
    "                            df = pd.DataFrame(pd.read_excel(xlsx_file))\n",
    "                            table_content = df.to_markdown().replace(\"_x000D_\", \"      \")\n",
    "                            parsed_file.write(table_content)\n",
    "                            parsed_file.write(\"\\n\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            text_content = element[\"Text\"]\n",
    "                            parsed_file.write(text_content)\n",
    "                            parsed_file.write(\"\\n\")\n",
    "                        except KeyError as ke:\n",
    "                            pass\n",
    "            parsed_file.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.exception(\"Exception encountered while executing operation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/public/home/llm2/yule/PDFTriage/data/test/chunk/file_0', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_2', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_5', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_7', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_9', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_1', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_3', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_4', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_6', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_8', '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_10']\n",
      "[Document(page_content='How Does Batch Normalization Help Optimization? \\narXiv:1805.11604v5  [stat.ML]  15 Apr 2019 \\n|    | Shibani Santurkar∗          | Dimitris Tsipras∗          | Andrew Ilyas∗          | Aleksander M ˛      adry          |\\n|---:|:-----------------------------|:----------------------------|:------------------------|:------------------------------------|\\n|  0 | MIT                         | MIT                        | MIT                    | MIT                                |\\n|  1 | shibani@mit.edu             | tsipras@mit.edu            | ailyas@mit.edu         | madry@mit.edu                      |\\nAbstract \\nBatch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. \\n1 Introduction \\nOver the last decade, deep learning has made impressive progress on a variety of notoriously difficult tasks in computer vision [(<>)16, (<>)7], speech recognition [(<>)5], machine translation [(<>)29], and game-playing [(<>)18, (<>)25]. This progress hinged on a number of major advances in terms of hardware, datasets [(<>)15, (<>)23], and algorithmic and architectural techniques [(<>)27, (<>)12, (<>)20, (<>)28]. One of the most prominent examples of such advances was batch normalization (BatchNorm) [(<>)10]. \\nAt a high level, BatchNorm is a technique that aims to improve the training of neural networks by stabilizing the distributions of layer inputs. This is achieved by introducing additional network layers that control the first two moments (mean and variance) of these distributions. \\nThe practical success of BatchNorm is indisputable. By now, it is used by default in most deep learning models, both in research (more than 6,000 citations) and real-world settings. Somewhat shockingly, however, despite its prominence, we still have a poor understanding of what the effectiveness of BatchNorm is stemming from. In fact, there are now a number of works that provide alternatives to BatchNorm [(<>)1, (<>)3, (<>)13, (<>)31], but none of them seem to bring us any closer to understanding this issue. (A similar point was also raised recently in [(<>)22].) \\nCurrently, the most widely accepted explanation of BatchNorm’s success, as well as its original motivation, relates to so-called internal covariate shift (ICS). Informally, ICS refers to the change in the distribution of layer inputs caused by updates to the preceding layers. It is conjectured that such continual change negatively impacts training. The goal of BatchNorm was to reduce ICS and thus remedy this effect. \\nEven though this explanation is widely accepted, we seem to have little concrete evidence supporting it. In particular, we still do not understand the link between ICS and training performance. The chief goal of this paper is to address all these shortcomings. Our exploration lead to somewhat startling discoveries. \\n∗Equal contribution. \\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada. \\nOur Contributions. Our point of start is demonstrating that there does not seem to be any link between the performance gain of BatchNorm and the reduction of internal covariate shift. Or that this link is tenuous, at best. In fact, we find that in a certain sense BatchNorm might not even be reducing internal covariate shift. \\nWe then turn our attention to identifying the roots of BatchNorm’s success. Specifically, we demonstrate that BatchNorm impacts network training in a fundamental way: it makes the landscape of the corresponding optimization problem significantly more smooth. This ensures, in particular, that the gradients are more predictive and thus allows for use of larger range of learning rates and faster network convergence. We provide an empirical demonstration of these findings as well as their theoretical justification. We prove that, under natural conditions, the Lipschitzness of both the loss and the gradients (also known as β-smoothness [(<>)21]) are improved in models with BatchNorm. \\nFinally, we find that this smoothening effect is not uniquely tied to BatchNorm. A number of other natural normalization techniques have a similar (and, sometime, even stronger) effect. In particular, they all offer similar improvements in the training performance. \\nWe believe that understanding the roots of such a fundamental techniques as BatchNorm will let us have a significantly better grasp of the underlying complexities of neural network training and, in turn, will inform further algorithmic progress in this context. \\nOur paper is organized as follows. In Section (<>)2, we explore the connections between BatchNorm, optimization, and internal covariate shift. Then, in Section (<>)3, we demonstrate and analyze the exact roots of BatchNorm’s success in deep neural network training. We present our theoretical analysis in Section (<>)4. We discuss further related work in Section (<>)5 and conclude in Section (<>)6. \\n2 Batch normalization and internal covariate shift \\nBatch normalization (BatchNorm) [(<>)10] has been arguably one of the most successful architectural innovations in deep learning. But even though its effectiveness is indisputable, we do not have a firm understanding of why this is the case. \\nBroadly speaking, BatchNorm is a mechanism that aims to stabilize the distribution (over a mini-batch) of inputs to a given network layer during training. This is achieved by augmenting the network with additional layers that set the first two moments (mean and variance) of the distribution of each activation to be zero and one respectively. Then, the batch normalized inputs are also typically scaled and shifted based on trainable parameters to preserve model expressivity. This normalization is applied before the non-linearity of the previous layer. \\nOne of the key motivations for the development of BatchNorm was the reduction of so-called internal covariate shift (ICS). This reduction has been widely viewed as the root of BatchNorm’s success. Ioffe and Szegedy [(<>)10] describe ICS as the phenomenon wherein the distribution of inputs to a layer in the network changes due to an update of parameters of the previous layers. This change leads to a constant shift of the underlying training problem and is thus believed to have detrimental effect on the training process. \\nFigure 1: Comparison of (a) training (optimization) and (b) test (generalization) performance of a standard VGG network trained on CIFAR-10 with and without BatchNorm (details in Appendix (<>)A). There is a consistent gain in training speed in models with BatchNorm layers. (c) Even though the gap between the performance of the BatchNorm and non-BatchNorm networks is clear, the difference in the evolution of layer input distributions seems to be much less pronounced. (Here, we sampled activations of a given layer and visualized their distribution over training steps.) \\nDespite its fundamental role and widespread use in deep learning, the underpinnings of BatchNorm’s success remain poorly understood [(<>)22]. In this work we aim to address this gap. To this end, we start by investigating the connection between ICS and BatchNorm. Specifically, we consider first training a standard VGG [(<>)26] architecture on CIFAR-10 [(<>)15] with and without BatchNorm. As expected, Figures (<>)1(a) and (b) show a drastic improvement, both in terms of optimization and generalization performance, for networks trained with BatchNorm layers. Figure (<>)1(c) presents, however, a surprising finding. In this figure, we visualize to what extent BatchNorm is stabilizing distributions of layer inputs by plotting the distribution (over a batch) of a random input over training. Surprisingly, the difference in distributional stability (change in the mean and variance) in networks with and without BatchNorm layers seems to be marginal. This observation raises the following questions: \\n(1) \\nIs the effectiveness of BatchNorm indeed related to internal covariate shift? \\n(2) \\nIs BatchNorm’s stabilization of layer input distributions even effective in reducing ICS? \\nWe now explore these questions in more depth. \\n2.1 Does BatchNorm’s performance stem from controlling internal covariate shift? \\nThe central claim in [(<>)10] is that controlling the mean and variance of distributions of layer inputs is directly connected to improved training performance. Can we, however, substantiate this claim? \\nWe propose the following experiment. We train networks with random noise injected after BatchNorm layers. Specifically, we perturb each activation for each sample in the batch using i.i.d. noise sampled from a non-zero mean and non-unit variance distribution. We emphasize that this noise distribution changes at each time step (see Appendix (<>)A for implementation details). \\nNote that such noise injection produces a severe covariate shift that skews activations at every time step. Consequently, every unit in the layer experiences a different distribution of inputs at each time step. We then measure the effect of this deliberately introduced distributional instability on BatchNorm’s performance. Figure (<>)2 visualizes the training behavior of standard, BatchNorm and our “noisy” BatchNorm networks. Distributions of activations over time from layers at the same depth in each one of the three networks are shown alongside. \\nObserve that the performance difference between models with BatchNorm layers, and “noisy” Batch-Norm layers is almost non-existent. Also, both these networks perform much better than standard networks. Moreover, the “noisy” BatchNorm network has qualitatively less stable distributions than even the standard, non-BatchNorm network, yet it still performs better in terms of training. To put \\nFigure 2: Connections between distributional stability and BatchNorm performance: We compare VGG networks trained without BatchNorm (Standard), with BatchNorm (Standard + BatchNorm) and with explicit “covariate shift” added to BatchNorm layers (Standard + “Noisy” BatchNorm). In the later case, we induce distributional instability by adding time-varying, non-zero mean and non-unit variance noise independently to each batch normalized activation. The “noisy” BatchNorm model nearly matches the performance of standard BatchNorm model, despite complete distributional instability. We sampled activations of a given layer and visualized their distributions (also cf. Figure (<>)7). \\n(a) VGG (b) DLN \\nFigure 3: Measurement of ICS (as defined in Definition (<>)2.1) in networks with and without BatchNorm layers. For a layer we measure the cosine angle (ideally 1) and \\ue0602-difference of the gradients (ideally 0) before and after updates to the preceding layers (see Definition (<>)2.1). Models with BatchNorm have similar, or even worse, internal covariate shift, despite performing better in terms of accuracy and loss. (Stabilization of BatchNorm faster during training is an artifact of parameter convergence.) \\nthe magnitude of the noise into perspective, we plot the mean and variance of random activations for select layers in Figure (<>)7. Moreover, adding the same amount of noise to the activations of the standard (non-BatchNorm) network prevents it from training entirely. \\nClearly, these findings are hard to reconcile with the claim that the performance gain due to Batch-Norm stems from increased stability of layer input distributions. \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_0'})]\n",
      "[Document(page_content='3.1 The smoothing effect of BatchNorm \\nIndeed, we identify the key impact that BatchNorm has on the training process: it reparametrizes the underlying optimization problem to make its landscape significantly more smooth. The first manifestation of this impact is improvement in the Lipschitzness2 (<>)of the loss function. That is, the loss changes at a smaller rate and the magnitudes of the gradients are smaller too. There is, however, \\n2Recall that f is L-Lipschitz if |f(x1) − f(x2)| ≤ L\\ue06bx1 − x2\\ue06b, for all x1 and x2. \\nan even stronger effect at play. Namely, BatchNorm’s reparametrization makes gradients of the loss more Lipschitz too. In other words, the loss exhibits a significantly better “effective” β-smoothness3 (<>). \\nThese smoothening effects impact the performance of the training algorithm in a major way. To understand why, recall that in a vanilla (non-BatchNorm), deep neural network, the loss function is not only non-convex but also tends to have a large number of “kinks”, flat regions, and sharp minima [(<>)17]. This makes gradient descent–based training algorithms unstable, e.g., due to exploding or vanishing gradients, and thus highly sensitive to the choice of the learning rate and initialization. \\nNow, the key implication of BatchNorm’s reparametrization is that it makes the gradients more reliable and predictive. After all, improved Lipschitzness of the gradients gives us confidence that when we take a larger step in a direction of a computed gradient, this gradient direction remains a fairly accurate estimate of the actual gradient direction after taking that step. It thus enables any (gradient–based) training algorithm to take larger steps without the danger of running into a sudden change of the loss landscape such as flat region (corresponding to vanishing gradient) or sharp local minimum (causing exploding gradients). This, in turn, enables us to use a broader range of (and thus larger) learning rates (see Figure (<>)10 in Appendix (<>)B) and, in general, makes the training significantly faster and less sensitive to hyperparameter choices. (This also illustrates how the properties of BatchNorm that we discussed earlier can be viewed as a manifestation of this smoothening effect.) \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_2'})]\n",
      "[Document(page_content='4.1 Setup \\nWe analyze the impact of adding a single BatchNorm layer after an arbitrary fully-connected layer W at a given step during the training. Specifically, we compare the optimization landscape of the original training problem to the one that results from inserting the BatchNorm layer after the fully-connected layer – normalizing the output of this layer (see Figure (<>)5). Our analysis therefore captures effects that stem from the reparametrization of the landscape and not merely from normalizing the inputs x. \\nWe denote the layer weights (identical for both the standard and batch-normalized networks) as Wij . Both networks have the same arbitrary loss function L that could potentially include a number of additional non-linear layers after the current one. We refer to the loss of the normalized network as we have an additional set of activations yˆ, which are the “whitened” version of y, i.e. standardized and γ to be constants for our analysis. In terms of notation, we let σj denote the standard deviation (computed over the mini-batch) of a batch of outputs yj ∈ Rm . L for clarity. In both networks, we have input x, and let y = W x. For networks with BatchNorm, \\ue062to mean 0 and variance 1. These are then multiplied by γ and added to β to form z. We assume β \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_5'})]\n",
      "[Document(page_content='A.1 Models \\nWe use two standard deep architectures – a VGG-like network, and a deep linear network (DLN). The VGG model achieves close to state-of-the-art performance while being fairly simple4 (<>). Preliminary experiments on other architectures gave similar results. We study DLNs with full-batch training since they allow us to isolate the effect of non-linearities, as well as the stochasticity of the training procedure. Both these architectures show clear a performance benefits with BatchNorm. \\nSpecific details regarding both architectures are provided below: \\n1. Convolutional VGG architecture on CIFAR10 (VGG): \\nWe fit a VGG-like network, a standard convolutional architecture [(<>)26], to a canonical image classification problem (CIFAR10 [(<>)15]). We optimize using standard stochastic gradient descent and train for 15, 000 steps (training accuracy plateaus). We use a batch size of 128 and a fixed learning rate of 0.1 unless otherwise specified. Moreover, since our focus is on training, we do not use data augmentation. This architecture can fit the training dataset well and achieves close to state-of-the art test performance. Our network achieves a test accuracy of 83% with BatchNorm and 80% without (this becomes 92% and 88% respectively with data augmentation). \\n2. 25-Layer Deep Linear Network on Synthetic Gaussian Data (DLN): \\nDLN are a factorized approach to solving a simple regression problem, i.e., fitting Ax from x. Specifically, we consider a deep network with k fully connected layers and an \\ue0602 loss. Thus, we are minimizing \\ue06bW1 . . . Wkx − Ax\\ue06b22 over Wi 5 (<>). We generate inputs x from a Gaussian Distribution and a matrix A with i.i.d. Gaussian entries. We choose k to be 25, and the dimensions of A to be 10 × 10. All the matrices Wi are square and have the same dimensions. We train DLN using full-batch gradient descent for 10, 000 steps (training loss plateaus).The size of the dataset is 1000 (same as the batch size) and the learning rate is 10−6 unless otherwise specified. \\nFor both networks we use standard Glorot initialization [(<>)4]. Further the learning rates were selected based on hyperparameter optimization to find a configuration where the training performance for the network was the best. \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_7'})]\n",
      "[Document(page_content='C.1 Useful facts and setup \\nWe consider the same setup pictured in Figure (<>)5 and described in Section (<>)4.1. Note that in proving the theorems we use partial derivative notation instead of gradient notation, and also rely on a few simple but key facts: \\n∂f\\n∂A(b) \\nf := f(C), \\nC = γ · B + β, \\n0,1\\nA−µ \\nσ \\nB = BN(A) := \\nFact C.1 (Gradient through BatchNorm). The gradient through BN and another function where and where A(b) are scalar elements of a batch of size m and variance σ2 is \\n∂f ∂A(b) = γ mσ \\ue020 m ∂f ∂C(b) − m\\ue058 k=1 ∂f ∂C(k) − B(b) m\\ue058 k=1 ∂f ∂C(k) B(k) \\ue021 \\nFact C.2 (Gradients of normalized outputs). A convenient gradient of BN is given as \\n∂ˆy(b) ∂y(k) = 1 σ \\ue012 1[b = k] − 1 m − 1 m ˆy(b) ˆy(k) \\ue013 , \\n(1) \\nand thus \\n∂z(b) j ∂y(k) = γ σ \\ue012 1[b = k] − 1 m − 1 m ˆy(b) ˆy(k) \\ue013 , \\n(2) \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_9'})]\n",
      "[Document(page_content='2.2 Is BatchNorm reducing internal covariate shift? \\nOur findings in Section (<>)2.1 make it apparent that ICS is not directly connected to the training performance, at least if we tie ICS to stability of the mean and variance of input distributions. One might wonder, however: Is there a broader notion of internal covariate shift that has such a direct link to training performance? And if so, does BatchNorm indeed reduce this notion? \\nRecall that each layer can be seen as solving an empirical risk minimization problem where given a set of inputs, it is optimizing some loss function (that possibly involves later layers). An update to the parameters of any previous layer will change these inputs, thus changing this empirical risk minimization problem itself. This phenomenon is at the core of the intuition that Ioffe and Szegedy [(<>)10] provide regarding internal covariate shift. Specifically, they try to capture this phenomenon from the perspective of the resulting distributional changes in layer inputs. However, as demonstrated in Section (<>)2.1, this perspective does not seem to properly encapsulate the roots of BatchNorm’s success. \\nTo answer this question, we consider a broader notion of internal covariate shift that is more tied to the underlying optimization task. (After all the success of BatchNorm is largely of an optimization nature.) Since the training procedure is a first-order method, the gradient of the loss is the most natural object to study. To quantify the extent to which the parameters in a layer would have to “adjust” in reaction to a parameter update in the previous layers, we measure the difference between the gradients of each layer before and after updates to all the previous layers. This leads to the following definition. \\nDefinition 2.1. Let L be the loss, W1(t) , . . . , Wk(t) be the parameters of each of the k layers and (x(t) , y(t)) be the batch of input-label pairs used to train the network at time t. We define internal covariate shift (ICS) of activation i at time t to be the difference ||Gt,i − Gt,i \\ue030 ||2, where \\nGt,i =∇ W (t) i L(W (t) 1 , . . . ,W (t) k ;x(t) ,y(t)) G \\ue030 t,i =∇ W (t) i L(W (t+1) 1 , . . . ,W (t+1) i−1 ,W (t) i ,W (t) i+1, . . . ,W (t) k ;x(t) ,y(t)). \\nHere, Gt,i corresponds to the gradient of the layer parameters that would be applied during a simultaneous update of all layers (as is typical). On the other hand, Gt,i \\ue030 is the same gradient after all \\n(a) loss landscape \\n(b) gradient predictiveness \\n(c) “effective” β-smoothness \\nFigure 4: Analysis of the optimization landscape of VGG networks. At a particular training step, we measure the variation (shaded region) in loss (a) and \\ue0602 changes in the gradient (b) as we move in the gradient direction. The “effective” β-smoothness (c) refers to the maximum difference (in \\ue0602-norm) in gradient over distance moved in that direction. There is a clear improvement in all of these measures in networks with BatchNorm, indicating a more well-behaved loss landscape. (Here, we cap the maximum distance to be η = 0.4× the gradient since for larger steps the standard network just performs worse (see Figure (<>)1). BatchNorm however continues to provide smoothing for even larger distances.) Note that these results are supported by our theoretical findings (Section (<>)4). \\nthe previous layers have been updated with their new values. The difference between G and G\\ue030 thus reflects the change in the optimization landscape of Wi caused by the changes to its input. It thus captures precisely the effect of cross-layer dependencies that could be problematic for training. \\nEquipped with this definition, we measure the extent of ICS with and without BatchNorm layers. To isolate the effect of non-linearities as well as gradient stochasticity, we also perform this analysis on (25-layer) deep linear networks (DLN) trained with full-batch gradient descent (see Appendix (<>)A for details). The conventional understanding of BatchNorm suggests that the addition of BatchNorm layers in the network should increase the correlation between G and G\\ue030 , thereby reducing ICS. \\nSurprisingly, we observe that networks with BatchNorm often exhibit an increase in ICS (cf. Figure (<>)3). This is particularly striking in the case of DLN. In fact, in this case, the standard network experiences almost no ICS for the entirety of training, whereas for BatchNorm it appears that G and G\\ue030 are almost uncorrelated. We emphasize that this is the case even though BatchNorm networks continue to perform drastically better in terms of attained accuracy and loss. (The stabilization of the BatchNorm VGG network later in training is an artifact of faster convergence.) This evidence suggests that, from optimization point of view BatchNorm might not even reduce the internal covariate shift. \\n3 Why does BatchNorm work? \\nOur investigation so far demonstrated that the generally asserted link between the internal covariate shift (ICS) and the optimization performance is tenuous, at best. But BatchNorm does significantly improve the training process. Can we explain why this is the case? \\nAside from reducing ICS, Ioffe and Szegedy [(<>)10] identify a number of additional properties of BatchNorm. These include prevention of exploding or vanishing gradients, robustness to different settings of hyperparameters such as learning rate and initialization scheme, and keeping most of the activations away from saturation regions of non-linearities. All these properties are clearly beneficial to the training process. But they are fairly simple consequences of the mechanics of BatchNorm and do little to uncover the underlying factors responsible for BatchNorm’s success. Is there a more fundamental phenomenon at play here? \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_1'})]\n",
      "[Document(page_content='3.2 Exploration of the optimization landscape \\nTo demonstrate the impact of BatchNorm on the stability of the loss itself, i.e., its Lipschitzness, for each given step in the training process, we compute the gradient of the loss at that step and measure how the loss changes as we move in that direction – see Figure (<>)4(a). We see that, in contrast to the case when BatchNorm is in use, the loss of a vanilla, i.e., non-BatchNorm, network has a very wide range of values along the direction of the gradient, especially in the initial phases of training. (In the later stages, the network is already close to convergence.) \\nSimilarly, to illustrate the increase in the stability and predictiveness of the gradients, we make analogous measurements for the \\ue0602 distance between the loss gradient at a given point of the training and the gradients corresponding to different points along the original gradient direction. Figure (<>)4(b) shows a significant difference (close to two orders of magnitude) in such gradient predictiveness between the vanilla and BatchNorm networks, especially early in training. \\nTo further demonstrate the effect of BatchNorm on the stability/Lipschitzness of the gradients of the loss, we plot in Figure (<>)4(c) the “effective” β-smoothness of the vanilla and BatchNorm networks throughout the training. (“Effective” refers here to measuring the change of gradients as we move in the direction of the gradients.). Again, we observe consistent differences between these networks. We complement the above examination by considering linear deep networks: as shown in Figures (<>)9 and (<>)12 in Appendix (<>)B, the BatchNorm smoothening effect is present there as well. \\nFinally, we emphasize that even though our explorations were focused on the behavior of the loss along the gradient directions (as they are the crucial ones from the point of view of the training process), the loss behaves in a similar way when we examine other (random) directions too. \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_3'})]\n",
      "[Document(page_content='3.3 Is BatchNorm the best (only?) way to smoothen the landscape? \\nGiven this newly acquired understanding of BatchNorm and the roots of its effectiveness, it is natural to wonder: Is this smoothening effect a unique feature of BatchNorm? Or could a similar effect be achieved using some other normalization schemes? \\nTo answer this question, we study a few natural data statistics-based normalization strategies. Specifically, we study schemes that fix the first order moment of the activations, as BatchNorm does, and then normalizes them by the average of their \\ue060p-norm (before shifting the mean), for p = 1, 2, ∞. Note that for these normalization schemes, the distributions of layer inputs are no longer Gaussian-like (see Figure (<>)14). Hence, normalization with such \\ue060p-norm does not guarantee anymore any control over the distribution moments nor distributional stability. \\n3Recall that f is β-smooth if its gradient is β-Lipschitz. It is worth noting that, due to the existence of non-linearities, one should not expect the β-smoothness to be bounded in an absolute, global sense. \\nyx W L \\n(a) Vanilla Network \\nyt − μ σ ̂yyx γ ̂y+ β z BatchNorm W bL (b) Vanilla Network + BatchNorm Layer \\nFigure 5: The two network architectures we compare in our theoretical analysis: (a) the vanilla DNN (no BatchNorm layer); (b) the same network as in (a) but with a BatchNorm layer inserted after the fully-connected layer W . (All the layer parameters have exactly the same value in both networks.) \\nThe results are presented in Figures (<>)13, (<>)11 and (<>)12 in Appendix (<>)B. We observe that all the normalization strategies offer comparable performance to BatchNorm. In fact, for deep linear networks, \\ue0601– normalization performs even better than BatchNorm. Note that, qualitatively, the \\ue060p–normalization techniques lead to larger distributional shift (as considered in [(<>)10]) than the vanilla, i.e., unnormalized, networks, yet they still yield improved optimization performance. Also, all these techniques result in an improved smoothness of the landscape that is similar to the effect of BatchNorm. (See Figures (<>)11 and (<>)12 of Appendix (<>)B.) This suggests that the positive impact of BatchNorm on training might be somewhat serendipitous. Therefore, it might be valuable to perform a principled exploration of the design space of normalization schemes as it can lead to better performance. \\n4 Theoretical Analysis \\nOur experiments so far suggest that BatchNorm has a fundamental effect on the optimization landscape. We now explore this phenomenon from a theoretical perspective. To this end, we consider an arbitrary linear layer in a DNN (we do not necessitate that the entire network be fully linear). \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_4'})]\n",
      "[Document(page_content='4.2 Theoretical Results \\nWe begin by considering the optimization landscape with respect to the activations yj . We show that batch normalization causes this landscape to be more well-behaved, inducing favourable properties in Lipschitz-continuity, and predictability of the gradients. We then show that these improvements in the activation-space landscape translate to favorable worst-case bounds in the weight-space landscape. \\n\\ue00c\\ue00c \\ue00c\\ue00c∇ L \\ue00c\\ue00c \\ue00c\\ue00c \\nWe first turn our attention to the gradient magnitude yj , which captures the Lipschitzness of the loss. The Lipschitz constant of the loss plays a crucial role in optimization, since it controls the amount by which the loss can change when taking a step (see [(<>)21] for details). Without any assumptions on the specific weights or the loss being used, we show that the batch-normalized \\nlandscape exhibits a better Lipschitz constant. Moreover, the Lipschitz constant is significantly \\ue062 reduced whenever the activations yˆj correlate with the gradient ∇yˆj L or the mean of the gradient deviates from 0. Note that this reduction is additive, and has effect even when the scaling of BN is identical to the original layer scaling (i.e. even when σj = γ). \\nTheorem 4.1 (The effect of BatchNorm on the Lipschitzness of the loss). For a BatchNorm network with loss L\\ue062 and an identical non-BN network with (identical) loss L, \\n\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c∇ yj \\ue062L \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c 2 ≤ γ2 σ2 j \\ue012 \\ue00c\\ue00c \\ue00c\\ue00c∇ yj L \\ue00c\\ue00c \\ue00c\\ue00c2 − 1 m \\ue00a 1, ∇ yj L \\ue00b2 − 1 m \\ue00a ∇ yj L, ˆyj \\ue00b2 \\ue013 . \\nγ \\nσ\\nFirst, note that \\ue0681, ∂L/∂y\\ue0692 grows quadratically in the dimension, so the middle term above is significant. Furthermore, the final inner product term is expected to be bounded away from zero, as the gradient with respect to a variable is rarely uncorrelated to the variable itself. In addition to the additive reduction, σj tends to be large in practice (cf. Appendix Figure (<>)8), and thus the scaling by may contribute to the relative “flatness\" we see in the effective Lipschitz constant. \\nWe now turn our attention to the second-order properties of the landscape. We show that when a BatchNorm layer is added, the quadratic form of the loss Hessian with respect to the activations in the gradient direction, is both rescaled by the input variance (inducing resilience to mini-batch variance), and decreased by an additive factor (increasing smoothness). This term captures the second order term of the Taylor expansion of the gradient around the current point. Therefore, reducing this term implies that the first order term (the gradient) is more predictive. \\nj \\nyj \\nˆg= ∇L \\njj \\n∂L\\n∂y\\nj ∂yj \\nH= \\nTheorem 4.2 (The effect of BN to smoothness). Let and be the gradient and Hessian of the loss with respect to the layer outputs respectively. Then \\n\\ue010 ∇yj \\ue062L \\ue011 \\ue03e ∂ \\ue062L∂yj ∂yj \\ue010 ∇yj \\ue062L \\ue011 ≤ γ 2 σ2 \\ue020 ∂ \\ue062L∂yj \\ue021 \\ue03e Hjj \\ue020 ∂ \\ue062L∂yj \\ue021 − γ mσ2 \\ue068ˆgj , ˆyj\\ue069 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 \\nIf we also have that the Hjj preserves the relative norms of gˆj and ∇yj \\ue062L, \\ue00c\\ue00c\\n\\ue010 ∇yj \\ue062L \\ue011 \\ue03e ∂ \\ue062L∂yj ∂yj \\ue010 ∇yj \\ue062L \\ue011 ≤ γ 2 σ2 \\ue020 ˆg\\ue03e j Hjj ˆgj − 1 mγ \\ue068ˆgj , ˆyj\\ue069 \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 \\ue021 \\nNote that if the quadratic forms involving the Hessian and the inner product \\ue068yˆj, gˆj\\ue069 are non-negative (both fairly mild assumptions), the theorem implies more predictive gradients. The Hessian is positive semi-definite (PSD) if the loss is locally convex which is true for the case of deep networks with piecewise-linear activation functions and a convex loss at the final layer (e.g. standard softmax cross-entropy loss or other common losses). The condition \\ue068yˆj, gˆj \\ue069 > 0 holds as long as the negative gradient gˆj is pointing towards the minimum of the loss (w.r.t. normalized activations). Overall, as long as these two conditions hold, the steps taken by the BatchNorm network are more predictive than those of the standard network (similarly to what we observed experimentally). \\nNote that our results stem from the reparametrization of the problem and not a simple scaling. Observation 4.3 (BatchNorm does more than rescaling). For any input data X and network configuration W , there exists a BN configuration (W, γ, β) that results in the same activations yj, and where γ = σj. Consequently, all of the minima of the normal landscape are preserved in the BN landscape. \\nOur theoretical analysis so far studied the optimization landscape of the loss w.r.t. the normalized activations. We will now translate these bounds to a favorable worst-case bound on the landscape with respect to layer weights. Note that a (near exact) analogue of this theorem for minimax gradient predictiveness appears in Theorem (<>)C.1 of Appendix (<>)C. \\nTheorem 4.4 (Minimax bound on weight-space Lipschitzness). For a BatchNorm network with loss L and an identical non-BN network (with identical loss L), if \\ue062\\ngj = max ||X||≤λ ||∇W L|| 2 , ˆgj = max ||X||≤λ \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c \\ue00c∇W \\ue062L \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c 2 =⇒ ˆgj ≤ γ 2 σ2 j \\ue010 g2 j − mµ2 gj −λ 2 \\ue00a ∇yj L, ˆyj \\ue00b 2 \\ue011 . \\nFinally, in addition to a desirable landscape, we find that BN also offers an advantage in initialization: \\n\\ue063 Lemma 4.5 (BatchNorm leads to a favourable initialization). Let W ∗ and W ∗ be the set of local optima for the weights in the normal and BN networks, respectively. For any initialization W0 \\n\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00cW0 −\\ue063W ∗ \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c 2 ≤ ||W0 −W ∗ || 2 − 1 ||W ∗|| 2 \\ue010 ||W ∗ || 2 − \\ue068W ∗ , W0\\ue069 \\ue0112 , \\nif \\ue068W0,\\ue063W ∗\\ue069 > 0, where W ∗ and W ∗ are closest optima for BN and standard network, respectively. \\n5 Related work \\nA number of normalization schemes have been proposed as alternatives to BatchNorm, including normalization over layers [(<>)1], subsets of the batch [(<>)31], or across image dimensions [(<>)30]. Weight Normalization [(<>)24] follows a complementary approach normalizing the weights instead of the activations. Finally, ELU [(<>)3] and SELU [(<>)13] are two proposed examples of non-linearities that have a progressively decaying slope instead of a sharp saturation and can be used as an alternative for BatchNorm. These techniques offer an improvement over standard training that is comparable to that of BatchNorm but do not attempt to explain BatchNorm’s success. \\nAdditionally, work on topics related to DNN optimization has uncovered a number of other Batch-Norm benefits. Li et al. [(<>)9] observe that networks with BatchNorm tend to have optimization trajectories that rely less on the parameter initialization. Balduzzi et al. [(<>)2] observe that models without BatchNorm tend to suffer from small correlation between different gradient coordinates and/or unit activations. They report that this behavior is profound in deeper models and argue how it constitutes an obstacle to DNN optimization. Morcos et al. [(<>)19] focus on the generalization properties of DNN. They observe that the use of BatchNorm results in models that rely less on single directions in the activation space, which they find to be connected to the generalization properties of the model. \\nRecent work [(<>)14] identifies simple, concrete settings where a variant of training with BatchNorm provably improves over standard training algorithms. The main idea is that decoupling the length and direction of the weights (as done in BatchNorm and Weight Normalization [(<>)24]) can be exploited to a large extent. By designing algorithms that optimize these parameters separately, with (different) adaptive step sizes, one can achieve significantly faster convergence rates for these problems. \\n6 Conclusions \\nIn this work, we have investigated the roots of BatchNorm’s effectiveness as a technique for training deep neural networks. We find that the widely believed connection between the performance of BatchNorm and the internal covariate shift is tenuous, at best. In particular, we demonstrate that existence of internal covariate shift, at least when viewed from the – generally adopted – distributional stability perspective, is not a good predictor of training performance. Also, we show that, from an optimization viewpoint, BatchNorm might not be even reducing that shift. \\nInstead, we identify a key effect that BatchNorm has on the training process: it reparametrizes the underlying optimization problem to make it more stable (in the sense of loss Lipschitzness) and smooth (in the sense of “effective” β-smoothness of the loss). This implies that the gradients used in training are more predictive and well-behaved, which enables faster and more effective optimization. This phenomena also explains and subsumes some of the other previously observed benefits of BatchNorm, such as robustness to hyperparameter setting and avoiding gradient explosion/vanishing. We also show that this smoothing effect is not unique to BatchNorm. In fact, several other natural normalization strategies have similar impact and result in a comparable performance gain. \\nWe believe that these findings not only challenge the conventional wisdom about BatchNorm but also bring us closer to a better understanding of this technique. We also view these results as an opportunity to encourage the community to pursue a more systematic investigation of the algorithmic toolkit of deep learning and the underpinnings of its effectiveness. \\nFinally, our focus here was on the impact of BatchNorm on training but our findings might also shed some light on the BatchNorm’s tendency to improve generalization. Specifically, it could be the case that the smoothening effect of BatchNorm’s reparametrization encourages the training process to converge to more flat minima. Such minima are believed to facilitate better generalization [(<>)8, (<>)11]. We hope that future work will investigate this intriguing possibility. \\nAcknowledgements \\nWe thank Ali Rahimi and Ben Recht for helpful comments on a preliminary version of this paper. \\nShibani Santurkar was supported by the National Science Foundation (NSF) under grants IIS-1447786, IIS-1607189, and CCF-1563880, and the Intel Corporation. Dimitris Tsipras was supported in part by the NSF grant CCF-1553428 and the NSF Frontier grant CNS-1413920. Andrew Ilyas was supported in part by NSF awards CCF-1617730 and IIS-1741137, a Simons Investigator Award, a Google Faculty Research Award, and an MIT-IBM Watson AI Lab research grant. Aleksander M ˛adry was supported in part by an Alfred P. Sloan Research Fellowship, a Google Research Award, and the NSF grants CCF-1553428 and CNS-1815221. \\nReferences \\n[1] \\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. In arXiv preprint arXiv:1607.06450, 2016. \\n[2] \\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? International Conference on Machine Learning (ICML), 2017. \\n[3] \\nDjork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deepnetwork learning by exponential linear units (elus). In International Conference on Learning Representations (ICLR), 2016. \\n[4] \\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, 2010. \\n[5] \\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In International Conference on Acoustics, Speech, and Signal Processing (IEEE-ICASSP), 2013. \\n[6] \\nMoritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on Learning Representations (ICLR), 2017. \\n[7] \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. \\n[8] \\nSepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 1997. \\n[9] \\nDaniel Jiwoong Im, Michael Tao, and Kristin Branson. An empirical analysis of deep network loss surfaces. arXiv preprint arXiv:1612.04010, 2016. \\n[10] \\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015. \\n[11] \\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on Learning Representations (ICLR), 2017. \\n[12] \\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. \\n[13] \\nGünter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In Advances in Neural Information Processing Systems (NIPS), 2017. \\n[14] \\nJonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hofmann. Towards a theoretical understanding of batch normalization. arXiv preprint arXiv:1805.10694, 2018. \\n[15] \\nAlex Krizhevsky. Learning multiple layers of features from tiny images. In Technical report, 2009. \\n[16] \\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012. \\n[17] \\nHao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems (NeurIPS), 2018. \\n[18] \\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 2015. \\n[19] \\nAri S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. In International Conference on Learning Representations (ICLR), 2018. \\n[20] \\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In International Conference on Machine Learning (ICML), 2010. \\n[21] \\nYurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Science & Business Media, 2014. \\n[22] \\nAli Rahimi and Ben Recht. Back when we were kids. NIPS Test of Time Award, 2017. \\n[23] \\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. In International Journal of Computer Vision (IJCV), 2015. \\n[24] \\nTim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems (NIPS), 2016. \\n[25] \\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016. \\n[26] \\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015. \\n[27] \\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research (JMLR), 2014. \\n[28] \\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the Importance of Initialization and Momentum in Deep Learning. In International Conference on Acoustics, Speech and Signal Processing (IEEE-ICASSP), 2013. \\n[29] \\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NIPS), 2014. \\n[30] \\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. \\n[31] \\nYuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision (ECCV), 2018. \\nA Experimental Setup \\nIn Section (<>)A.1, we provide details regarding the architectures used in our analysis. Then in Section (<>)A.2 we discuss the specifics of the setup and measurements used in our experiments. \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_6'})]\n",
      "[Document(page_content='A.2 Details \\nA.2.1 “Noisy” BatchNorm Layers \\nConsider ai,j , the j-th activation of the i-th example in the batch. Note that batch norm will ensure that the distribution of a·,j for some j will have fixed mean and variance (possibly learnable). \\nAt every time step, our noise model consists of perturbing each activation for each sample in a batch with noise i.i.d. from a non-zero mean, non-unit variance distribution Djt . The distribution Dt itself is time varying and its parameters are drawn i.i.d from another distribution Dj . The specific noise j model is described in Algorithm (<>)1. In our experiments, nµ = 0.5, nσ = 1.25 and rµ = rσ = 0.1. (For convolutional layers, we follow the standard convention of treating the height and width dimensions as part of the batch.) \\n4We choose to not experiment with ResNets [(<>)7] since they seem to provide several similar benefits to BatchNorm \\n[(<>)6] and would introduce conflating factors into our study. \\n5While the factorized formulation is equivalent to a single matrix in terms of expressivity, the optimization landscape is drastically different [(<>)6]. \\nAlgorithm 1 “Noisy” BatchNorm \\n1: \\n% For constants nm, nv, rm, rv. \\n2: \\n3: \\nfor each layer at time t do \\n4: \\nati,j ← Batch-normalized activation for unit j and sample i \\n5: \\n6: \\nfor each j do \\n7: \\nµ ∼ U(−nµ, nµ) 8: σt ∼ U(1, nσ) t \\n9: \\n10: \\nfor each i do \\n11: \\nfor each j do \\n12: \\nmti,j ∼ U(µ − rµ, µ + rµ) \\n13: \\nsti,j ∼ N (σ, rσ) \\n14: \\nat ← st t i,j i,j · ai,j + mi,j \\n\\ue02e Sample the parameters (mtj, vjt) of Djt from Dj \\n\\ue02e Sample noise from Djt \\nWhile plotting the distribution of activations, we sample random activations from any given layer of the network and plot its distribution over the batch dimension for fully connected layers, and over the batch, height, width dimension for convolutional layers as is standard convention in BatchNorm for convolutional networks. \\nA.2.2 Loss Landscape \\nTo measure the smoothness of the loss landscape of a network during the course of training, we essentially take steps of different lengths in the direction of the gradient and measure the loss values obtained at each step. Note that this is not a training procedure, but an evaluation of the local loss landscape at every step of the training process. \\nFor VGG we consider steps of length ranging from [1/2, 4] × step size, whereas for DLN we choose [1/100, 30] × step size. Here step size denotes the hyperparameter setting with which the network is being trained. We choose these ranges to roughly reflect the range of parameters that are valid for standard training of these models. The VGG network is much more sensitive to the learning rate choices (probably due to the non-linearities it includes), so we perform line search over a restricted range of parameters. Further, the maximum step size was chosen slightly smaller than the learning rate at which the standard (no BatchNorm) network diverges during training. \\nB Omitted Figures \\nAdditional visualizations for the analysis performed in Section (<>)3.1 are presented below. \\n(a) VGG (b) DLN \\nFigure 6: Measurement of ICS (as defined in Definition (<>)2.1) in networks with and without BatchNorm layers. For a layer we measure the cosine angle (ideally 1) and \\ue0602-difference of the gradients (ideally 0) before and after updates to the preceding layers (see Definition (<>)2.1). Models with BatchNorm have similar, or even worse, internal covariate shift, despite performing better in terms of accuracy and loss. (Stabilization of BatchNorm faster during training is an artifact of parameter convergence.) \\n(a) (b) \\nFigure 7: Comparison of change in the first two moments (mean and variance) of distributions of example activations for a given layer between two successive steps of the training process. Here we compare VGG networks trained without BatchNorm (Standard), with BatchNorm (Standard + BatchNorm) and with explicit “covariate shift” added to BatchNorm layers (Standard + “Noisy” BatchNorm). “Noisy” BatchNorm layers have significantly higher ICS than standard networks, yet perform better from an optimization perspective (cf. Figure (<>)2). \\nFigure 8: Distributions of activations from different layers of a 25-Layer deep linear network. Here we sample a random activation from a given layer to visualize its distribution over training. \\n(a) loss landscape (b) gradient predictiveness (c) “effective” β-smoothness \\nFigure 9: Analysis of the optimization landscape during training of deep linear networks with and without BatchNorm. At a particular training step, we measure the variation (shaded region) in loss (a) and \\ue0602 changes in the gradient (b) as we move in the gradient direction. The “effective” β-smoothness (c) captures the maximum β value observed while moving in this direction. There is a clear improvement in each of these measures of smoothness of the optimization landscape in networks with BatchNorm layers. (Here, we cap the maximum distance moved to be η = 30× the gradient since for larger steps the standard network just performs works (see Figure (<>)1). However, BatchNorm continues to provide smoothing for even larger distances.) \\n(a) VGG (b) DLN \\nFigure 10: Comparison of the predictiveness of gradients with and without BatchNorm. Here, at a given step in the optimization, we measure the \\ue0602 error between the current gradient, and new gradients which are observed while moving in the direction of the current gradient. We then evaluate how this error varies based on distance traversed in the direction of the gradient. We observe that gradients are significantly more predictive in networks with BatchNorm and change slowly in a given local neighborhood. This explains why networks with BatchNorm are largely robust to a broad range of learning rates. \\nFigure 11: Evaluation of VGG networks trained with different \\ue060p normalization strategies discussed in Section (<>)3.3. (a): Comparison of the training performance of the models. (b, c, d): Evaluation of the smoothness of optimization landscape in the various models. At a particular training step, we measure the variation (shaded region) in loss (b) and \\ue0602 changes in the gradient (c) as we move in the gradient direction. We also measure the maximum β-smoothness while moving in this direction (d). We observe that networks with any normalization strategy have improved performance and smoothness of the loss landscape over standard training. \\nFigure 12: Evaluation of deep linear networks trained with different \\ue060p normalization strategies. We observe that networks with any normalization strategy have improved performance and smoothness of the loss landscape over standard training. Details of the plots are the same as Figure (<>)11 above. \\n(a) VGG \\n(b) Deep Linear Model \\nFigure 13: Evaluation of the training performance of \\ue060p normalization techniques discussed in Section (<>)3.3. For both networks, all \\ue060p normalization strategies perform comparably or even better than BatchNorm. This indicates that the performance gain with BatchNorm is not about distributional stability (controlling mean and variance). \\nFigure 14: Activation histograms for the VGG network under different normalizations. Here, we randomly sample activations from a given layer and visualize their distributions. Note that the \\ue060p -normalization techniques leads to larger distributional covariate shift compared to normal networks, yet yield improved optimization performance. \\nC Proofs \\nWe now prove the stated theorems regarding the landscape induced by batch normalization. \\nWe begin with a few facts that can be derived directly from the closed-form of Batch Normalization, which we use freely in proving the following theorems. \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_8'})]\n",
      "[Document(page_content='C.2 Lipschitzness proofs \\nNow, we provide a proof for the Lipschitzness of the loss landscape in terms of the layer activations. In particular, we prove the following theorem from Section (<>)4. \\nTheorem 4.1 (The effect of BatchNorm on the Lipschitzness of the loss). For a BatchNorm network with loss L\\ue062 and an identical non-BN network with (identical) loss L, \\n\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c∇ yj \\ue062L \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c 2 ≤ γ2 σ2 j \\ue012 \\ue00c\\ue00c \\ue00c\\ue00c∇ yj L \\ue00c\\ue00c \\ue00c\\ue00c2 − 1 m \\ue00a 1,∇ yj L \\ue00b2 − 1 m \\ue00a ∇ yj L, ˆyj \\ue00b2 \\ue013 . \\nProof. Proving this is simply a direct application of Fact (<>)C.1. In particular, we have that \\n∂ \\ue062L∂yj (b) = \\ue012 γ mσj \\ue013\\ue020 m ∂ \\ue062L∂z j (b) − m\\ue058 k=1 ∂ \\ue062L∂z j(k) − ˆy(b) j m\\ue058 k=1 ∂ \\ue062L∂z j (k) ˆy(k) j \\ue021 , \\n(3) \\nwhich we can write in vectorized form as \\n∂ \\ue062L∂yj = \\ue012 γ mσ j \\ue013\\ue020 m ∂ \\ue062L∂zj −1 \\ue02a 1, ∂ \\ue062L∂zj \\ue02b −ˆyj \\ue02a ∂ \\ue062L∂zj ,ˆyj \\ue02b\\ue021 \\n(4) \\ng \\n1 \\nm \\nj \\nµ= 1,∂ \\ue062L/∂z\\nj \\nˆy\\nNow, let be the mean of the gradient vector, we can rewrite the above as the √ following (in the subsequent steps taking advantage of the fact that is mean-zero and norm- m: \\ue045\\ue044 \\n∂ \\ue062L∂yj = \\ue012 γ σj \\ue013 \\ue020\\ue020 ∂ \\ue062L∂zj − 1µg \\ue021 − 1 m ˆyj \\ue02a\\ue020 ∂ \\ue062L∂zj − 1µg \\ue021 ,ˆyj \\ue02b\\ue021 = γ σ \\ue020\\ue020 ∂ \\ue062L∂zj − 1µg \\ue021 − ˆyj ||ˆyj || \\ue02a\\ue020 ∂ \\ue062L∂zj − 1µg \\ue021 , ˆyj ||ˆyj || \\ue02b\\ue021 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c2 = γ2 σ2 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue020 ∂ \\ue062L∂zj − 1µg \\ue021 − ˆyj ||ˆyj || \\ue02a\\ue020 ∂ \\ue062L∂zj − 1µg \\ue021 , ˆyj ||ˆyj || \\ue02b\\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 = γ2 σ2 ⎛ ⎝ \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue020 ∂ \\ue062L∂zj − 1µg \\ue021\\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 − \\ue02a\\ue020 ∂ \\ue062L∂zj − 1µg \\ue021 , ˆyj ||ˆyj || \\ue02b2 ⎞ ⎠ = γ2 σ2 ⎛ ⎝ \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂zj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 − 1 m \\ue02a 1, ∂ \\ue062L∂zj \\ue02b2 − 1 m \\ue02a ∂ \\ue062L∂zj ,ˆyj \\ue02b2 ⎞ ⎠ \\n(5) \\n(6) \\n(7) \\n(8) \\n(9) \\nExploiting the fact that ∂L\\ue062/∂zj = ∂L/∂y gives the desired result. \\nNext, we can use this to prove the minimax bound on the Lipschitzness with respect to the weights. Theorem 4.4 (Minimax bound on weight-space Lipschitzness). For a BatchNorm network with loss L and an identical non-BN network (with identical loss L), if \\ue062\\ngj = max ||X||≤λ ||∇W L|| 2 , ˆgj = max ||X||≤λ \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c \\ue00c∇W \\ue062L \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c 2 =⇒ ˆgj ≤ γ 2 σ2 j \\ue010 g 2 j − mµ 2 gj −λ 2 \\ue00a ∇yj L, ˆyj \\ue00b 2 \\ue011 . \\nProof. To prove this, we start with the following identity for the largest eigenvalue λ0 of M ∈ Rd×d: \\nλ0 = max x∈R d;||x||2=1 x \\ue03e Mx, \\n(10) \\nwhich in turn implies that for a matrix X with ||X||2 ≤ λ, it must be that v\\ue03eXv ≤ λ||v||2 , with the choice of X = λI making this bound tight. \\nNow, we derive the gradient with respect to the weights via the chain rule: \\n∂ \\ue062L∂W ij = m\\ue058 b=1 ∂ \\ue062L∂yj (b) ∂yj(b) ∂W ij \\n(11) \\n∂ \\ue062L∂Wij = m\\ue058 b=1 ∂ \\ue062L∂yj (b) xi (b) \\ue02a\\ue02b \\n(12) \\n= ∂ \\ue062L∂yj ,xi ∂ \\ue062L∂W·j =X \\ue03e \\ue020 ∂ \\ue062L∂yj \\ue021 , \\n(13) \\n(14) \\nwhere X ∈ Rm×d is the input matrix holding Xbi = xi(b) \\n. Thus, \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂W·j \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 = \\ue020 ∂ \\ue062L∂yj \\ue021\\ue03e XX \\ue03e \\ue020 ∂ \\ue062L ∂yj \\ue021 , \\n(15) \\nand since we have ||X||2 ≤ λ, we must have ||XX\\ue03e||2 ≤ λ2 , and so recalling ((<>)10), \\nmax ||X||2<λ \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂W·j \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 ≤λ 2\\ue020 ∂ \\ue062L∂yj \\ue021\\ue03e \\ue020 ∂ \\ue062L∂yj \\ue021 =λ 2 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 , \\n(16) \\nand applying Theorem (<>)4.1 yields: \\nˆgj := max ||X||2<λ \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂W·j \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 ≤ λ2γ2 σ2 \\ue020\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c ∂L ∂yj \\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c 2 − 1 m \\ue01c 1, ∂L ∂yj \\ue01d2 −1 √ m \\ue01c ∂L ∂yj ,ˆyj \\ue01d2 \\ue021 . \\n(17) \\nFinally, by applying ((<>)10) again, note that in fact in the normal network, \\ngj := max ||X||2<λ \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂W·j \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 =λ 2 \\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c ∂L ∂yj \\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c 2 , \\n(18) \\nand thus \\nˆgj ≤ γ2 σ2 \\ue020 g2 j − mµ2 gj −λ 2 \\ue01c ∂L ∂yj ,ˆyj \\ue01d2 \\ue021 . \\nj \\nyj \\nˆg= ∇L \\njj \\n∂L\\n∂y\\nj ∂yj \\nH= \\nTheorem 4.2 (The effect of BN to smoothness). Let and be the gradient and Hessian of the loss with respect to the layer outputs respectively. Then \\n\\ue010 ∇yj \\ue062L \\ue011 \\ue03e ∂ \\ue062L∂yj ∂yj \\ue010 ∇yj \\ue062L \\ue011 ≤ γ 2 σ2 \\ue020 ∂ \\ue062L∂yj \\ue021 \\ue03e Hjj \\ue020 ∂ \\ue062L∂yj \\ue021 − γ mσ2 \\ue068ˆgj , ˆyj\\ue069 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 \\nIf we also have that the Hjj preserves the relative norms of gˆj and ∇yj L\\ue062, \\n\\ue010 ∇yj \\ue062L \\ue011 \\ue03e ∂ \\ue062L∂yj ∂yj \\ue010 ∇yj \\ue062L \\ue011 ≤ γ 2 σ2 \\ue020 ˆg\\ue03e j Hjj ˆgj − 1 mγ \\ue068ˆgj , ˆyj\\ue069 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 \\ue021 \\nProof. We use the following notation freely in the following. First, we introduce the hessian with respect to the final activations as: \\nHjk ∈ Rm×m; Hjk := ∂ \\ue062L∂zj ∂zk = ∂L ∂yj ∂yk , \\n(v) \\n1 \\nd\\n\\ue03e\\nµ= 1v \\nd \\nv ∈ R\\n(·) \\n(·)\\nµ= µ1 \\nwhere the final equality is by the assumptions of our setup. Once again for convenience, we define a function µ(·) which operates on vectors and matrices and gives their element-wise mean; in particular, for and we write to be a vector with all elements equal to µ. Finally, we denote the gradient with respect to the batch-normalized outputs as gˆj , such that: \\nˆgj = ∂ \\ue062L∂zj = ∂L ∂yj , \\nwhere again the last equality is by assumption. \\nNow, we begin by looking at the Hessian of the loss with respect to the pre-BN activations yj using the expanded gradient as above: \\n∂ \\ue062L∂yj ∂yj = ∂ ∂yj \\ue012\\ue012 γ mσ j \\ue013\\ue068 mˆgj − mµ(ˆgj ) − ˆy(b) j \\ue068ˆgj , ˆyj\\ue069 \\ue069\\ue013 Using the product rule and the chain rule: = γ mσ \\ue012 ∂ ∂z q \\ue002 mˆgj − mµ(ˆgj ) − ˆyj \\ue068ˆgj , ˆyj \\ue069 \\ue003 \\ue013 · ∂z q ∂yj + \\ue012 ∂ ∂yj \\ue012 γ mσ j \\ue013\\ue013 · \\ue000 mˆgj − mµ(ˆgj ) − ˆyj \\ue068ˆgj , ˆyj \\ue069 \\ue001 \\n(19) \\n(20) \\n(21) \\nDistributing the derivative across subtraction: = \\ue012 γ σj \\ue013\\ue012 Hjj − ∂µ(ˆgj ) ∂zj − ∂ ∂zj \\ue012 1 m ˆyj \\ue068ˆgj , ˆyj\\ue069 \\ue013\\ue013 · ∂zj ∂yj + \\ue012 ˆgj − µ(ˆgj ) − 1 m ˆyj \\ue068ˆgj , ˆyj \\ue069 \\ue013\\ue012 ∂ ∂yj \\ue012 γ σ j \\ue013\\ue013 \\n(22) \\n(23) \\nWe address each of the terms in the above ((<>)22) and ((<>)23) one by one: \\n∂µ(ˆgj ) ∂zj = 1 m ∂1\\ue03eˆgj ∂zj = 1 m 1·1 \\ue03e H jj \\n(24) \\n∂ ∂zj (ˆyj \\ue068ˆyj, ˆgj \\ue069) = 1 γ ∂ ∂ˆyj (ˆyj \\ue068ˆyj, ˆgj\\ue069) \\n(25) = 1 γ ∂ˆyj ∂ˆyj \\ue068ˆgj, ˆyj\\ue069 + ˆyj ˆy \\ue03e j Hjj + 1 γ ˆyj ˆg \\ue03e j ∂ˆyj ∂ ˆyj (26) = 1 γ I \\ue068ˆgj , ˆyj \\ue069 + ˆyj ˆy \\ue03e j H jj + 1 γ ˆyj ˆg \\ue03e j I (27) ∂ ∂yj \\ue012 γ σj \\ue013 =γ √ m ∂ \\ue010\\ue000 yj − µ(yj ) \\ue001\\ue03e \\ue000 yj − µ(yj ) \\ue001\\ue011−12 ∂yj (28) = −1 2 γ √ m \\ue000 (yj − µ(yj ))\\ue03e(yj − µ(yj )) \\ue001−32(2(yj − µ(yj ))) (29) =− γ mσ2 ˆyj (30) \\nNow, we can use the preceding to rewrite the Hessian as: \\n∂ \\ue062L∂yj ∂yj = \\ue012 γ mσ j \\ue013\\ue012 mHjj − 1 · 1 \\ue03e Hjj − 1 γ I \\ue068ˆgj , ˆyj \\ue069 − ˆyj ˆy \\ue03e j Hjj − 1γ \\ue000 ˆyj ˆg \\ue03e j \\ue001 \\ue013 · ∂zj ∂yj (31) − γ mσ2 \\ue012 ˆgj − µ(ˆgj ) − 1 m ˆyj \\ue068ˆgj , ˆyj\\ue069 \\ue013 ˆy\\ue03e j (32) \\nNow, using Fact (<>)C.2, we have that: \\n∂zj ∂yj = \\ue012 γ σ j \\ue013\\ue012 I− 1 m 1·1 \\ue03e− 1 m ˆyj ˆy \\ue03e j \\ue013 , \\n(33) \\nand substituting this yields (letting M = 1 · 1\\ue03e for convenience): \\n∂ \\ue062L∂yj ∂yj = γ2 mσ2 \\ue012 mH jj − MH jj − 1 γ I \\ue068ˆgj, ˆyj \\ue069 − ˆyj ˆy \\ue03e j H jj − 1γ \\ue000 ˆyj ˆg \\ue03e j \\ue001 \\ue013 (34) − γ2 mσ2 \\ue012 HjjM − 1 m MHjjM − 1 mγ M \\ue068ˆgj , ˆyj \\ue069 − 1 m ˆyj ˆy \\ue03e j HjjM − 1 mγ \\ue000 ˆyjˆg \\ue03e j M \\ue001 \\ue013 \\n(35) − γ2 mσ2 \\ue012 H jj ˆyj ˆy \\ue03e j − 1 m MH jj ˆyj ˆy \\ue03e j − 1 mγ ˆyj ˆy \\ue03e j \\ue068ˆgj, ˆyj \\ue069 − 1 m ˆyj ˆy \\ue03e j H jj ˆyj ˆy \\ue03ej − 1 mγ \\ue000 ˆyj ˆg \\ue03e j ˆyj ˆy \\ue03e j \\ue001 \\ue013 (36) \\n− γ mσ2 \\ue012 ˆgj − µ(ˆgj ) − 1 m ˆyj \\ue068ˆgj , ˆyj\\ue069 \\ue013 ˆy \\ue03e j \\n(37) \\nCollecting the terms, and letting ˆgj =ˆgj − µ(ˆgj ): ∂ \\ue062L∂yj ∂yj = γ2 mσ2 \\ue014 mH jj − MH jj − ˆyj ˆy \\ue03e j H jj −H jj M+ 1 m MH jj M (38) + 1 m ˆyj ˆy \\ue03e j H jj M −H jj ˆyj ˆy \\ue03e j + 1 m MH jj ˆyj ˆy \\ue03e j + 1 m ˆyj ˆy \\ue03e j H jj ˆyj ˆy \\ue03e j \\ue015 (39) − γ mσ2 \\ue012 ˆgj ˆy \\ue03e j − µ(ˆgj ) ˆy \\ue03e j − 3 m ˆyj ˆy \\ue03e j \\ue068ˆgj , ˆyj\\ue069 + \\ue000 \\ue068ˆgj, ˆyj\\ue069 I + ˆyj ˆg \\ue03e j \\ue001 \\ue012 I− 1 m M \\ue013\\ue013 (40) = γ2 σ2 \\ue014\\ue012 I− 1 m ˆyj ˆy \\ue03e j − 1 m M \\ue013 Hjj \\ue012 I− 1 m ˆyj ˆy \\ue03e j − 1 m M \\ue013 (41) − 1 mγ \\ue012 ˆgj ˆy \\ue03e j + ˆyj ˆgj \\ue03e − 3 m ˆyj ˆg \\ue03e j ˆyˆy \\ue03e j + \\ue068ˆgj, ˆyj \\ue069 \\ue012 I− 1 m M \\ue013\\ue013\\ue015 (42) \\nNow, we wish to calculate the effective beta smoothness with respect to a batch of activations, which \\ue03ecorresponds to g Hg, where g is the gradient with respect to the activations (as derived in the previous proof). We expand this product noting the following identities: \\nMˆgj = 0 \\ue012 I− 1 m M− 1 m ˆyj ˆy \\ue03e j \\ue0132 = \\ue012 I− 1 m M− 1 m ˆyj ˆy \\ue03e j \\ue013 ˆy\\ue03e j \\ue012 I− 1 m ˆyj ˆy \\ue03e j \\ue013 =0 \\ue012 I− 1 m M \\ue013\\ue012 I− 1 m ˆyj ˆy \\ue03e j \\ue013 ˆgj = \\ue012 I− 1 m ˆyj ˆy \\ue03e j \\ue013 ˆgj \\n(43) \\n(44) \\n(45) \\n(46) \\nAlso recall from ((<>)5) that: \\n∂ \\ue062L∂yj = γ σ ˆgj \\ue03e \\ue012 I− 1 m ˆyj ˆy\\ue03e j \\ue013 \\n(47) \\nApplying these while expanding the product gives: \\n∂ \\ue062L∂yj \\ue03e · ∂ \\ue062L∂yj ∂yj · ∂ \\ue062L∂yj = γ4 σ4 ˆgj \\ue03e \\ue012 I− 1 mˆyj ˆy \\ue03e j \\ue013 Hjj \\ue012 I− 1 m ˆyj ˆy \\ue03e j \\ue013 ˆgj − γ3 mσ4 ˆgj \\ue03e \\ue012 I− 1 m ˆyj ˆy \\ue03e j \\ue013 ˆgj \\ue068ˆgj , ˆyj \\ue069 = γ2 σ2 \\ue020 ∂ \\ue062L∂yj \\ue021\\ue03e H jj \\ue020 ∂ \\ue062L∂yj \\ue021 − γ mσ2 \\ue068ˆgj , ˆyj \\ue069 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 \\n(48) \\n(49) \\n(50) \\n∂ \\ue062\\nL\\n∂\\nyj \\njj \\n\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c2 \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c H, \\nThis concludes the first part of the proof. Note that if Hjj preserves the relative norms of gˆj and ∇yj L, then the final statement follows trivially, since the first term of the above is simply the induced squared norm and so \\ue062 \\n∂ \\ue062L∂yj \\ue03e · ∂ \\ue062L∂yj ∂yj · ∂ \\ue062L∂yj ≤ γ2 σ2 ⎡ ⎣ˆg \\ue03e j H jj ˆgj − 1 mγ \\ue068ˆgj, ˆyj \\ue069 \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c ∂ \\ue062L∂yj \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c\\ue00c\\ue00c 2 ⎤ ⎦\\n(51) \\nOnce again, the same techniques also give us a minimax separation: \\nTheorem C.1 (Minimax smoothness bound). Under the same conditions as the previous theorem, \\nmax ||X||≤λ \\ue020 ∂ \\ue062L∂W·j \\ue021 \\ue03e ∂ \\ue062L∂W·j ∂W·j \\ue020 ∂ \\ue062L∂W·j \\ue021 < γ 2 σ2 \\ue022 max ||X||≤λ \\ue012 ∂L ∂W·j \\ue013 \\ue03e ∂L ∂W·j ∂W·j \\ue012 ∂L ∂W·j \\ue013 −λ 4 κ \\ue023 , \\nwhere κ is the separation given in the previous theorem. \\nProof. \\n∂L ∂W ij ∂W kj =x \\ue03e i ∂L ∂yj ∂yj xk ∂ \\ue062L∂W ij ∂W kj =x \\ue03e i ∂ \\ue062L∂yj ∂yj xk ∂ \\ue062L∂W·j ∂W·j =X \\ue03e ∂ \\ue062L∂yj ∂yj X \\n(52) \\n(53) \\n(54) \\n(55) \\nLooking at the gradient predictiveness using the gradient we derived in the first proofs: \\nβ := \\ue020 ∂ \\ue062L∂W·j \\ue021\\ue03e ∂ \\ue062L∂W·j ∂W·j \\ue020 ∂ \\ue062L∂W·j \\ue021 = ˆg\\ue03e j \\ue012 I− 1 m ˆyj ˆy\\ue03e j \\ue013 XX \\ue03e ∂ \\ue062L∂yj ∂yj XX \\ue03e \\ue012 I− 1 m ˆyj ˆy\\ue03e j \\ue013 ˆgj \\n(56) \\n(57) \\nMaximizing the norm with respect to X yields: \\nmax ||X||≤λ β = λ 4 ˆg\\ue03e j \\ue012 I− 1 m ˆyj ˆy\\ue03e j \\ue013 ∂ \\ue062L∂yj∂yj \\ue012 I− 1 m ˆyj ˆy\\ue03e j \\ue013 ˆgj , \\n(58) \\nat which the previous proof can be applied to conclude. \\n\\ue063 Lemma 4.5 (BatchNorm leads to a favourable initialization). Let W ∗ and W ∗ be the set of local optima for the weights in the normal and BN networks, respectively. For any initialization W0 \\n\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00cW0 −\\ue063W ∗ \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c 2 ≤ ||W0 − W ∗ || 2 − 1 ||W∗|| 2 \\ue010 ||W ∗ || 2 − \\ue068W ∗ ,W0\\ue069 \\ue0112 , \\nif \\ue068W0,W\\ue063∗ \\ue069 > 0, where W∗ and W∗ are closest optima for BN and standard network, respectively. \\nProof. This is as a result of the scale-invariance of batch normalization. In particular, first note that for any optimum W in the standard network, we have that any scalar multiple of W must also be an optimum in the BN network (since BN((aW)x) = BN(Wx) for all a > 0). Recall that we have defined k > 0 to be propertial to the correlation between W0 and W∗: \\nk= \\ue068W ∗ ,W0\\ue069 ||W∗|| 2 \\nThus, for any optimum W∗ , we must\\ue063have that W := kW∗ must be an optimum in the BN network. The difference between distance to this optimum and the distance to W is given by: \\n\\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00cW0 −\\ue063W \\ue00c\\ue00c\\ue00c \\ue00c\\ue00c\\ue00c 2 − ||W0 − W ∗ || 2 = ||W0 − kW ∗ || 2 − || W 0 − W ∗ || 2 (59) = \\ue010 ||W0|| 2 −k 2 ||W ∗ || 2 \\ue011 − \\ue010 ||W0|| 2 − 2k ||W ∗ || 2 + ||W ∗ || 2 \\ue011 (60) = 2k ||W ∗ || 2 −k 2 ||W ∗ || 2 − ||W ∗ || 2 (61) = − ||W ∗ || 2 · (1 − k)2 (62) \\n', metadata={'source': '/public/home/llm2/yule/PDFTriage/data/test/chunk/file_10'})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_extract_client_id = '8f2bfd9259224ac285feaae73e813c2f'\n",
    "    pdf_extract_client_secret = 'p8e-Ou_2-JQP04FYX00-BAO6Hk8_M0_X4aMr'\n",
    "\n",
    "    input_file_path = \"/public/home/llm2/yule/PDFTriage/data/test.pdf\"\n",
    "    output_path = \"/public/home/llm2/yule/PDFTriage/data/test.zip\"\n",
    "    unzip_dir = \"/public/home/llm2/yule/PDFTriage/data/test/unzip\"\n",
    "    chunked_dir = \"/public/home/llm2/yule/PDFTriage/data/test/chunk\"\n",
    "    isExist = os.path.exists(chunked_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(chunked_dir)\n",
    "    pdf_extract = PDFExtract(pdf_extract_client_id, pdf_extract_client_secret)\n",
    "\n",
    "    # Step - 1 : Run this step to chunk the PDF into contextual subsections\n",
    "\n",
    "    pdf_extract.parse_pdf(input_file_path,output_path,unzip_dir,chunked_dir)\n",
    "\n",
    "    # Step - 2 : use a TextLoader to get all the chunks in a list of Dcoments\n",
    "\n",
    "    files = pdf_extract.get_files_from_dir(chunked_dir)\n",
    "    print(files)\n",
    "    list_of_all_docs=[]\n",
    "    for file in files:\n",
    "        document = pdf_extract.load_docs(file)\n",
    "\n",
    "        list_of_all_docs.append(document[0])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'extended_metadata', 'elements', 'pages'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_path = '/public/home/llm2/yule/PDFTriage/data/test/unzip/structuredData.json'\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "            content = json.loads(json_file.read())\n",
    "content.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//Document/Table/TR/TH[4]/P'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['elements'][10]['Path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
